{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as the supporting material for the chapter **Reinforcement Learning**. It illustrates the use of the [reinforcement](https://github.com/aimacode/aima-java/tree/AIMA3e/aima-core/src/main/java/aima/core/learning/reinforcement) package of the code repository. Here we'll examine how an agent can learn what to do in the absence of labeled examples of what to do, from rewards and punishments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4478f207-b147-4b3e-aca2-34a8cfe6db14",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add jar ../out/artifacts/aima_core_jar/aima-core.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps; for example, maximize the points won in a game over many moves. They can start from a blank slate, and under the right conditions they achieve superhuman performance. \n",
    "\n",
    "Consider an example of a problem of learning chess. A supervised agent needs to be told the correct move for each position it encounters, but such feedback is seldom available. Therefore, in the absence of feedback, the agent needs to know, that something good has happened when it accidentally checkmates its opponent and that something bad has happened when it gets checkmated. This kind of feedback is called a **reward** or **reinforcement**. Reinforcement learning differs from the supervised learning in a way that in supervised learning the training data has the answer label with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of the training dataset, it is bound to learn from its experience. \n",
    "\n",
    "Usually, in game playing, it is very hard for a human to provide accurate and consistent evaluations of a large number of positions. Therefore, the program is told when it has won or lost, and the agent uses this information to learn a reasonably accurate evaluation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some important concepts before proceeding further: \n",
    "\n",
    "* **Reward** ($R$): A reward is the feedback by which we measure the success or failure of an agent’s actions. From any given state, an agent sends output in the form of actions to the environment, and the environment returns the agent’s new state (which resulted from acting on the previous state) as well as rewards, if there are any. They effectively evaluate the agent’s action.\n",
    "* **Policy** ($\\pi$): The policy is the strategy that the agent employs to determine the next action based on the current state. It maps states to actions, the actions that promise the highest reward. The policy that yields the highest expected utility is known as **optimal policy**. We use $\\pi^*$ to denote an optimal policy.\n",
    "* **Discount factor** ($\\gamma$): The discount factor is multiplied by future rewards as discovered by the agent in order to dampen thse rewards’ effect on the agent’s choice of action. Why? It is designed to make future rewards worth less than immediate rewards. If $\\gamma$ is 0.8, and there’s a reward of 10 points after 3 time steps, the present value of that reward is 0.8³ x 10. A discount factor of 1 would make future rewards worth just as much as immediate rewards.\n",
    "* **Transition model**: The transition model describes the outcome of each action in each state. If the outcomes are stochastic, we write $P(s'|s,a)$ to denote the probability of reaching state $s'$ if the action $a$ is done in state $s$. We'll assume the transitions are **Markovian** i.e. the probability of reaching $s'$ from $s$ depends only on $s$ and not on the history of earlier states. \n",
    "* **Utility** ($U(s)$): The utility  is defined to be the expected sum of discounted rewards if the policy $\\pi$ is followed from that state onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In passive learning, the agent's policy $\\pi$ is fixed: in state $s$, it always executes the action $\\pi(s)$. It's goal is to learn how good a policy is - that is to learn a utility function $U^{\\pi}(s)$. Note that the passive learning agent does not know the transition model $P(s'|s,a)$, which specifies the probability of reaching state $s'$, from state $s$ after doing action $a$; nor does it know the reward function $R(s)$, which specifies the reward for each state. The agent executes a set of trials in the environment using its policy $\\pi$. In each trial, agent begins from the start-position and experience a sequence of state transition until it reaches one of the terminal states. Its percept supply both the current state and the reward receied in that state. The objective is to use the information about the rewards to learn the expected utility $U^{\\pi}(s)$ associated with each non-terminal state $s$. \n",
    "\n",
    "Since, the utility values obey the Bellman equation for a fixed policy $\\pi$, i.e. _the utility for each state equals its own reward  plus the expected utility of its successors states_,\n",
    "\n",
    "$U^{\\pi}(s) = R(s) + \\gamma\\sum_{s'}P(s' | s,\\pi(s))U^\\pi(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Dynamic Programming\n",
    "\n",
    "An adaptive dynamic programming agent takes advantage of the constraints among the utilities of states by learning the transition model that connects them and solving the corresponding Markov decision process using a dynamic programming method. For a passive learning agent, this means plugging a learned transition model $P(s'|s,\\pi(s))$ and the observed reward $R(s)$ into the Bellman equation to calculate the utilities of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the pseudo code of Passive ADP agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### AIMA3e\n",
       "__function__ Passive-ADP-Agent(_percept_) __returns__ and action  \n",
       "&emsp;__inputs__: _percept_, a percept indication the current state _s'_ and reward signal _r'_  \n",
       "&emsp;__persistent__: _&pi;_, a fixed policy  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_mdp_, an MDP with model _P_, rewards _R_, discount &gamma;  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_U_, a table of utilities, initially empty  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_N<sub>sa</sub>_, a table of frequencies for state-action pairs, initially zero  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_N<sub>s'|sa</sub>_, a table of outcome frequencies given state-action pairs, initially zero  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_s_, _a_, the previous state and action, initially null  \n",
       "&emsp;__if__ _s'_ is new __then__ _U_[_s'_] &larr; _r'_; _R_[_s'_] &larr; _r'_  \n",
       "&emsp;__if__ _s_ is not null __then__  \n",
       "&emsp;&emsp;&emsp;increment _N<sub>sa</sub>_[_s_, _a_] and _N<sub>s'|sa</sub>_[_s'_, _s_, _a_]  \n",
       "&emsp;&emsp;&emsp;__for each__ _t_ such that _N<sub>s'|sa</sub>_[_t_, _s_, _a_] is nonzero __do__  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;_P_(_t_ | _s_, _a_) &larr; _N<sub>s'|sa</sub>_[_t_, _s_, _a_] / _N<sub>sa</sub>_[_s_, _a_]  \n",
       "&emsp;_U_ &larr; Policy-Evaluation(_&pi;_, _U_, _mdp_)  \n",
       "&emsp;__if__ _s'_.Terminal? __then__ _s_, _a_ &larr; null __else__ _s_, _a_ &larr; _s'_, _&pi;_[_s'_]  \n",
       "\n",
       "---\n",
       "__Figure ??__ A passive reinforcement learning agent based on adaptive dynamic programming. The Policy-Evaluation function solves the fixed-policy Bellman equations, as described on page ??."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "from notebookUtils import *\n",
    "pseudocode('Passive ADP Agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our Passive ADP agent in action! Consider a $4*3$ cell world with $[1,1]$ as the starting position. The policy $\\pi$ for the $4*3$ world is shown in the figure below. This policy happens to be optimal with rewards of $R(s)=-0.04$ in the non-terminal states and no discounting.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Optimal Policy][1]][1]\n",
    "\n",
    "[1]: assets/optimal-policy.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell \t:\tExpected Utility\n",
      "-------------------------------------\n",
      "[1,1] \t:\t0.7153193259024824\n",
      "[1,2] \t:\t0.7707398421463386\n",
      "[1,3] \t:\t0.8203828048081079\n",
      "[2,1] \t:\t0.6670047267920397\n",
      "[2,3] \t:\t0.8762199960076309\n",
      "[3,1] \t:\tnull\n",
      "[3,2] \t:\t0.7344940650463005\n",
      "[3,3] \t:\t0.9266999990710265\n",
      "[4,1] \t:\tnull\n",
      "[4,2] \t:\t-1.0\n",
      "[4,3] \t:\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aima.core.environment.cellworld.*;\n",
    "import aima.core.learning.reinforcement.agent.PassiveADPAgent;\n",
    "import aima.core.learning.reinforcement.example.CellWorldEnvironment;\n",
    "import aima.core.probability.example.MDPFactory;\n",
    "import aima.core.probability.mdp.impl.ModifiedPolicyEvaluation;\n",
    "import aima.core.util.JavaRandomizer;\n",
    "\n",
    "import java.util.*;;\n",
    "\n",
    "CellWorld<Double> cw = CellWorldFactory.createCellWorldForFig17_1();;\n",
    "CellWorldEnvironment cwe = new CellWorldEnvironment(\n",
    "            cw.getCellAt(1, 1),\n",
    "            cw.getCells(),\n",
    "            MDPFactory.createTransitionProbabilityFunctionForFigure17_1(cw),\n",
    "            new JavaRandomizer());\n",
    "Map<Cell<Double>, CellWorldAction> fixedPolicy = new HashMap<Cell<Double>, CellWorldAction>();\n",
    "fixedPolicy.put(cw.getCellAt(1, 1), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(2, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(2, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(3, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(3, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(3, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(4, 1), CellWorldAction.Left);\n",
    "PassiveADPAgent<Cell<Double>, CellWorldAction> padpa = new PassiveADPAgent<Cell<Double>, CellWorldAction>(\n",
    "                                                                fixedPolicy,\n",
    "                                                                cw.getCells(), \n",
    "                                                                cw.getCellAt(1, 1), \n",
    "                                                                MDPFactory.createActionsFunctionForFigure17_1(cw),\n",
    "                                                                new ModifiedPolicyEvaluation<Cell<Double>, CellWorldAction>(10,1.0));\n",
    "cwe.addAgent(padpa);\n",
    "padpa.reset();\n",
    "cwe.executeTrials(2000);\n",
    "System.out.println(\"Cell\"  + \" \\t:\\t\" + \"Expected Utility\");\n",
    "System.out.println(\"-------------------------------------\");\n",
    "Map<Cell<Double>, Double> U = padpa.getUtility();\n",
    "for(int i = 1; i<=4; i++){\n",
    "    for(int j = 1; j<=3; j++){\n",
    "        if(i==2 && j==2) continue; //Ignore wall\n",
    "        System.out.println(\"[\" + i + \",\" + j + \"]\"  + \" \\t:\\t\" + U.get(cw.getCellAt(i,j)));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cells $[3,1]$ and $[4,1]$ are not reachable when starting at $[1,1]$ using the policy and the default transition model i.e. 80% intended and 10% each right angle from intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves of the Passive ADP agent for the $4*3$ world (given the optimal policy) are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aima.core.environment.cellworld.*;\n",
    "import aima.core.learning.reinforcement.agent.PassiveADPAgent;\n",
    "import aima.core.learning.reinforcement.example.CellWorldEnvironment;\n",
    "import aima.core.probability.example.MDPFactory;\n",
    "import aima.core.probability.mdp.impl.ModifiedPolicyEvaluation;\n",
    "import aima.core.util.JavaRandomizer;\n",
    "\n",
    "import java.util.*;\n",
    "\n",
    "int numRuns = 20;\n",
    "int numTrialsPerRun = 100;\n",
    "int rmseTrialsToReport = 100;\n",
    "int reportEveryN = 1;\n",
    "\n",
    "CellWorld<Double> cw = CellWorldFactory.createCellWorldForFig17_1();;\n",
    "CellWorldEnvironment cwe = new CellWorldEnvironment(\n",
    "            cw.getCellAt(1, 1),\n",
    "            cw.getCells(),\n",
    "            MDPFactory.createTransitionProbabilityFunctionForFigure17_1(cw),\n",
    "            new JavaRandomizer());\n",
    "Map<Cell<Double>, CellWorldAction> fixedPolicy = new HashMap<Cell<Double>, CellWorldAction>();\n",
    "fixedPolicy.put(cw.getCellAt(1, 1), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(2, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(2, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(3, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(3, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(3, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(4, 1), CellWorldAction.Left);\n",
    "PassiveADPAgent<Cell<Double>, CellWorldAction> padpa = new PassiveADPAgent<Cell<Double>, CellWorldAction>(\n",
    "                                                                fixedPolicy,\n",
    "                                                                cw.getCells(), \n",
    "                                                                cw.getCellAt(1, 1), \n",
    "                                                                MDPFactory.createActionsFunctionForFigure17_1(cw),\n",
    "                                                                new ModifiedPolicyEvaluation<Cell<Double>, CellWorldAction>(10,1.0));\n",
    "cwe.addAgent(padpa);\n",
    "Map<Integer, List<Map<Cell<Double>, Double>>> runs = new HashMap<Integer, List<Map<Cell<Double>, Double>>>();\n",
    "for (int r = 0; r < numRuns; r++) {\n",
    "    padpa.reset();\n",
    "    List<Map<Cell<Double>, Double>> trials = new ArrayList<Map<Cell<Double>, Double>>();\n",
    "    for (int t = 0; t < numTrialsPerRun; t++) {\n",
    "        cwe.executeTrial();\n",
    "        if (0 == t % reportEveryN) {\n",
    "            Map<Cell<Double>, Double> u = padpa.getUtility();\n",
    "            trials.add(u);\n",
    "        }\n",
    "    }\n",
    "    runs.put(r, trials);\n",
    "}\n",
    "\n",
    "def T = [];\n",
    "def v4_3 = [];\n",
    "def v3_3 = [];\n",
    "def v1_3 = [];\n",
    "def v1_1 = [];\n",
    "def v3_2 = [];\n",
    "def v2_1 = [];\n",
    "double tmp = 0.0;\n",
    "for (int t = 0; t < (numTrialsPerRun / reportEveryN); t++) {\n",
    "    T.add(t);\n",
    "    Map<Cell<Double>, Double> u = runs.get(numRuns - 1).get(t);\n",
    "    tmp = (u.containsKey(cw.getCellAt(4, 3)) ? u.get(cw.getCellAt(4, 3)) : 0.0);\n",
    "    v4_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(3, 3)) ? u.get(cw.getCellAt(3, 3)) : 0.0);\n",
    "    v3_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(1, 3)) ? u.get(cw.getCellAt(1, 3)) : 0.0);\n",
    "    v1_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(1, 1)) ? u.get(cw.getCellAt(1, 1)) : 0.0);\n",
    "    v1_1.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(3, 2)) ? u.get(cw.getCellAt(3, 2)) : 0.0);\n",
    "    v3_2.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(2, 1)) ? u.get(cw.getCellAt(2, 1)) : 0.0);\n",
    "    v2_1.add(tmp);\n",
    "}\n",
    "\n",
    "def p1 = new Plot(title: \"Learning Curve\", yLabel: \"Utility estimates\", xLabel: \"Number of trails\");\n",
    "p1 << new Line(x: T, y: v4_3, displayName: \"v4_3\")\n",
    "p1 << new Line(x: T, y: v3_3, displayName: \"v3_3\")\n",
    "p1 << new Line(x: T, y: v1_3, displayName: \"v1_3\")\n",
    "p1 << new Line(x: T, y: v1_1, displayName: \"v1_1\")\n",
    "p1 << new Line(x: T, y: v3_2, displayName: \"v3_2\")\n",
    "p1 << new Line(x: T, y: v2_1, displayName: \"v2_1\")\n",
    "\n",
    "def trails = [];\n",
    "def rmseValues = [];\n",
    "for (int t = 0; t < rmseTrialsToReport; t++) {\n",
    "    trails.add(t);\n",
    "    double xSsquared = 0;\n",
    "    for (int r = 0; r < numRuns; r++) {\n",
    "        Map<Cell<Double>, Double> u = runs.get(r).get(t);\n",
    "        Double val1_1 = u.get(cw.getCellAt(1, 1));\n",
    "        xSsquared += Math.pow(0.705 - val1_1, 2);\n",
    "    }\n",
    "    double rmse = Math.sqrt(xSsquared / runs.size());\n",
    "    rmseValues.add(rmse);\n",
    "}\n",
    "def p2 = new Plot(yLabel: \"RMS error in utility\", xLabel: \"Number of trails\");\n",
    "p2 << new Line(x: trails, y: rmseValues)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Utility estimates][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/utility_estimates.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![RMS error in utility][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/RMSerror.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first figure shows the utility estimates for some of the states as a function of the number of trails. Notice the large changes occuring around the 63rd trial - this is the first time that the agent falls into the -1 terminal state at $[4,2]$. \n",
    "* The second plot shoes the root-mean-square error in the estimate for $U(1,1)$, averaged over 20 runs of 100 trails each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Groovy",
   "language": "groovy",
   "name": "groovy"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": ".groovy",
   "mimetype": "",
   "name": "Groovy",
   "nbconverter_exporter": "",
   "version": "2.4.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
