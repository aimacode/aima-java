{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as the supporting material for the chapter **Reinforcement Learning**. It illustrates the use of the [reinforcement](https://github.com/aimacode/aima-java/tree/AIMA3e/aima-core/src/main/java/aima/core/learning/reinforcement) package of the code repository. Here we'll examine how an agent can learn what to do in the absence of labeled examples of what to do, from rewards and punishments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4478f207-b147-4b3e-aca2-34a8cfe6db14",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add jar ../out/artifacts/aima_core_jar/aima-core.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps; for example, maximize the points won in a game over many moves. They can start from a blank slate, and under the right conditions, they achieve superhuman performance. \n",
    "\n",
    "Consider an example of a problem of learning chess. A supervised agent needs to be told the correct move for each position it encounters, but such feedback is seldom available. Therefore, in the absence of feedback, the agent needs to know, that something good has happened when it accidentally checkmates its opponent and that something bad has happened when it gets checkmated. This kind of feedback is called a **reward** or **reinforcement**. Reinforcement learning differs from the supervised learning in a way that in supervised learning the training data has the answer label with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of the training dataset, it is bound to learn from its experience. \n",
    "\n",
    "Usually, in game playing, it is very hard for a human to provide accurate and consistent evaluations of a large number of positions. Therefore, the program is told when it has won or lost, and the agent uses this information to learn a reasonably accurate evaluation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some important concepts before proceeding further: \n",
    "\n",
    "* **Reward** ($R$): A reward is feedback by which we measure the success or failure of an agent’s actions. From any given state, an agent sends output in the form of actions to the environment, and the environment returns the agent’s new state (which resulted from acting on the previous state) as well as rewards if there are any. They effectively evaluate the agent’s action.\n",
    "* **Policy** ($\\pi$): The policy is the strategy that the agent employs to determine the next action based on the current state. It maps states to actions, the actions that promise the highest reward. The policy that yields the highest expected utility is known as an **optimal policy**. We use $\\pi^*$ to denote an optimal policy.\n",
    "* **Discount factor** ($\\gamma$): The discount factor is multiplied by future rewards as discovered by the agent to dampen these rewards’ effect on the agent’s choice of action. Why? It is designed to make future rewards worth less than immediate rewards. If $\\gamma$ is 0.8, and there’s a reward of 10 points after 3 time steps, the present value of that reward is 0.8³ x 10. A discount factor of 1 would make future rewards worth just as much as immediate rewards.\n",
    "* **Transition model**: The transition model describes the outcome of each action in each state. If the outcomes are stochastic, we write $P(s'|s,a)$ to denote the probability of reaching state $s'$ if the action $a$ is done in state $s$. We'll assume the transitions are **Markovian** i.e. the probability of reaching $s'$ from $s$ depends only on $s$ and not on the history of earlier states. \n",
    "* **Utility** ($U(s)$): The utility  is defined to be the expected sum of discounted rewards if the policy $\\pi$ is followed from that state onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In passive learning, the agent's policy $\\pi$ is fixed: in state $s$, it always executes the action $\\pi(s)$. Its goal is to learn how good a policy is - that is to learn a utility function $U^{\\pi}(s)$. Note that the passive learning agent does not know the transition model $P(s'|s,a)$, which specifies the probability of reaching state $s'$, from state $s$ after doing action $a$; nor does it know the reward function $R(s)$, which specifies the reward for each state. The agent executes a set of trials in the environment using its policy $\\pi$. In each trial, the agent begins from the start-position and experience a sequence of state transition until it reaches one of the terminal states. It's percept supply both the current state and the reward received in that state. The objective is to use the information about the rewards to learn the expected utility $U^{\\pi}(s)$ associated with each non-terminal state $s$. \n",
    "\n",
    "Since the utility values obey the Bellman equation for a fixed policy $\\pi$, i.e. _the utility for each state equals its own reward  plus the expected utility of its successors' states_,\n",
    "\n",
    "$U^{\\pi}(s) = R(s) + \\gamma\\sum_{s'}P(s' | s,\\pi(s))U^\\pi(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Dynamic Programming\n",
    "\n",
    "An adaptive dynamic programming agent takes advantage of the constraints among the utilities of states by learning the transition model that connects them and solving the corresponding Markov decision process using a dynamic programming method. For a passive learning agent, this means plugging a learned transition model $P(s'|s,\\pi(s))$ and the observed reward $R(s)$ into the Bellman equation to calculate the utilities of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the pseudo code of Passive ADP agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### AIMA3e\n",
       "__function__ Passive-ADP-Agent(_percept_) __returns__ and action  \n",
       "&emsp;__inputs__: _percept_, a percept indication the current state _s'_ and reward signal _r'_  \n",
       "&emsp;__persistent__: _&pi;_, a fixed policy  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_mdp_, an MDP with model _P_, rewards _R_, discount &gamma;  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_U_, a table of utilities, initially empty  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_N<sub>sa</sub>_, a table of frequencies for state-action pairs, initially zero  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_N<sub>s'|sa</sub>_, a table of outcome frequencies given state-action pairs, initially zero  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_s_, _a_, the previous state and action, initially null  \n",
       "&emsp;__if__ _s'_ is new __then__ _U_[_s'_] &larr; _r'_; _R_[_s'_] &larr; _r'_  \n",
       "&emsp;__if__ _s_ is not null __then__  \n",
       "&emsp;&emsp;&emsp;increment _N<sub>sa</sub>_[_s_, _a_] and _N<sub>s'|sa</sub>_[_s'_, _s_, _a_]  \n",
       "&emsp;&emsp;&emsp;__for each__ _t_ such that _N<sub>s'|sa</sub>_[_t_, _s_, _a_] is nonzero __do__  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;_P_(_t_ | _s_, _a_) &larr; _N<sub>s'|sa</sub>_[_t_, _s_, _a_] / _N<sub>sa</sub>_[_s_, _a_]  \n",
       "&emsp;_U_ &larr; Policy-Evaluation(_&pi;_, _U_, _mdp_)  \n",
       "&emsp;__if__ _s'_.Terminal? __then__ _s_, _a_ &larr; null __else__ _s_, _a_ &larr; _s'_, _&pi;_[_s'_]  \n",
       "\n",
       "---\n",
       "__Figure ??__ A passive reinforcement learning agent based on adaptive dynamic programming. The Policy-Evaluation function solves the fixed-policy Bellman equations, as described on page ??."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "from notebookUtils import *\n",
    "pseudocode('Passive ADP Agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our Passive ADP agent in action! Consider a $4*3$ cell world with $[1,1]$ as the starting position. The policy $\\pi$ for the $4*3$ world is shown in the figure below. This policy happens to be optimal with reward of $R(s)=-0.04$ in the non-terminal states and no discounting.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Optimal Policy][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/optimal_policy.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell \t:\tExpected Utility\n",
      "----------------------------------\n",
      "[1,1] \t:\t0.7081191789618011\n",
      "[1,2] \t:\t0.763739805626589\n",
      "[1,3] \t:\t0.8138902449639785\n",
      "[2,1] \t:\t0.6584738143530444\n",
      "[2,3] \t:\t0.8703399959976966\n",
      "[3,1] \t:\tnull\n",
      "[3,2] \t:\t0.6759847275839783\n",
      "[3,3] \t:\t0.9205199990813694\n",
      "[4,1] \t:\tnull\n",
      "[4,2] \t:\t-1.0\n",
      "[4,3] \t:\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aima.core.environment.cellworld.*;\n",
    "import aima.core.learning.reinforcement.agent.PassiveADPAgent;\n",
    "import aima.core.learning.reinforcement.example.CellWorldEnvironment;\n",
    "import aima.core.probability.example.MDPFactory;\n",
    "import aima.core.probability.mdp.impl.ModifiedPolicyEvaluation;\n",
    "import aima.core.util.JavaRandomizer;\n",
    "\n",
    "import java.util.*;;\n",
    "\n",
    "CellWorld<Double> cw = CellWorldFactory.createCellWorldForFig17_1();;\n",
    "CellWorldEnvironment cwe = new CellWorldEnvironment(\n",
    "            cw.getCellAt(1, 1),\n",
    "            cw.getCells(),\n",
    "            MDPFactory.createTransitionProbabilityFunctionForFigure17_1(cw),\n",
    "            new JavaRandomizer());\n",
    "Map<Cell<Double>, CellWorldAction> fixedPolicy = new HashMap<Cell<Double>, CellWorldAction>();\n",
    "fixedPolicy.put(cw.getCellAt(1, 1), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(2, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(2, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(3, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(3, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(3, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(4, 1), CellWorldAction.Left);\n",
    "PassiveADPAgent<Cell<Double>, CellWorldAction> padpa = new PassiveADPAgent<Cell<Double>, CellWorldAction>(\n",
    "                                                                fixedPolicy,\n",
    "                                                                cw.getCells(), \n",
    "                                                                cw.getCellAt(1, 1), \n",
    "                                                                MDPFactory.createActionsFunctionForFigure17_1(cw),\n",
    "                                                                new ModifiedPolicyEvaluation<Cell<Double>, CellWorldAction>(10,1.0));\n",
    "cwe.addAgent(padpa);\n",
    "padpa.reset();\n",
    "cwe.executeTrials(2000);\n",
    "System.out.println(\"Cell\"  + \" \\t:\\t\" + \"Expected Utility\");\n",
    "System.out.println(\"----------------------------------\");\n",
    "Map<Cell<Double>, Double> U = padpa.getUtility();\n",
    "for(int i = 1; i<=4; i++){\n",
    "    for(int j = 1; j<=3; j++){\n",
    "        if(i==2 && j==2) continue; //Ignore wall\n",
    "        System.out.println(\"[\" + i + \",\" + j + \"]\"  + \" \\t:\\t\" + U.get(cw.getCellAt(i,j)));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cells $[3,1]$ and $[4,1]$ are not reachable when starting at $[1,1]$ using the policy and the default transition model i.e. 80% intended and 10% each right angle from intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves of the Passive ADP agent for the $4*3$ world (given the optimal policy) are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aima.core.environment.cellworld.*;\n",
    "import aima.core.learning.reinforcement.agent.PassiveADPAgent;\n",
    "import aima.core.learning.reinforcement.example.CellWorldEnvironment;\n",
    "import aima.core.probability.example.MDPFactory;\n",
    "import aima.core.probability.mdp.impl.ModifiedPolicyEvaluation;\n",
    "import aima.core.util.JavaRandomizer;\n",
    "\n",
    "import java.util.*;\n",
    "\n",
    "int numRuns = 20;\n",
    "int numTrialsPerRun = 100;\n",
    "int rmseTrialsToReport = 100;\n",
    "int reportEveryN = 1;\n",
    "\n",
    "CellWorld<Double> cw = CellWorldFactory.createCellWorldForFig17_1();;\n",
    "CellWorldEnvironment cwe = new CellWorldEnvironment(\n",
    "            cw.getCellAt(1, 1),\n",
    "            cw.getCells(),\n",
    "            MDPFactory.createTransitionProbabilityFunctionForFigure17_1(cw),\n",
    "            new JavaRandomizer());\n",
    "Map<Cell<Double>, CellWorldAction> fixedPolicy = new HashMap<Cell<Double>, CellWorldAction>();\n",
    "fixedPolicy.put(cw.getCellAt(1, 1), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(2, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(2, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(3, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(3, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(3, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(4, 1), CellWorldAction.Left);\n",
    "PassiveADPAgent<Cell<Double>, CellWorldAction> padpa = new PassiveADPAgent<Cell<Double>, CellWorldAction>(\n",
    "                                                                fixedPolicy,\n",
    "                                                                cw.getCells(), \n",
    "                                                                cw.getCellAt(1, 1), \n",
    "                                                                MDPFactory.createActionsFunctionForFigure17_1(cw),\n",
    "                                                                new ModifiedPolicyEvaluation<Cell<Double>, CellWorldAction>(10,1.0));\n",
    "cwe.addAgent(padpa);\n",
    "Map<Integer, List<Map<Cell<Double>, Double>>> runs = new HashMap<Integer, List<Map<Cell<Double>, Double>>>();\n",
    "for (int r = 0; r < numRuns; r++) {\n",
    "    padpa.reset();\n",
    "    List<Map<Cell<Double>, Double>> trials = new ArrayList<Map<Cell<Double>, Double>>();\n",
    "    for (int t = 0; t < numTrialsPerRun; t++) {\n",
    "        cwe.executeTrial();\n",
    "        if (0 == t % reportEveryN) {\n",
    "            Map<Cell<Double>, Double> u = padpa.getUtility();\n",
    "            trials.add(u);\n",
    "        }\n",
    "    }\n",
    "    runs.put(r, trials);\n",
    "}\n",
    "\n",
    "def T = [];\n",
    "def v4_3 = [];\n",
    "def v3_3 = [];\n",
    "def v1_3 = [];\n",
    "def v1_1 = [];\n",
    "def v3_2 = [];\n",
    "def v2_1 = [];\n",
    "double tmp = 0.0;\n",
    "for (int t = 0; t < (numTrialsPerRun/reportEveryN); t++) {\n",
    "    T.add(t);\n",
    "    Map<Cell<Double>, Double> u = runs.get(numRuns - 1).get(t);\n",
    "    tmp = (u.containsKey(cw.getCellAt(4, 3)) ? u.get(cw.getCellAt(4, 3)) : 0.0);\n",
    "    v4_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(3, 3)) ? u.get(cw.getCellAt(3, 3)) : 0.0);\n",
    "    v3_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(1, 3)) ? u.get(cw.getCellAt(1, 3)) : 0.0);\n",
    "    v1_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(1, 1)) ? u.get(cw.getCellAt(1, 1)) : 0.0);\n",
    "    v1_1.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(3, 2)) ? u.get(cw.getCellAt(3, 2)) : 0.0);\n",
    "    v3_2.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(2, 1)) ? u.get(cw.getCellAt(2, 1)) : 0.0);\n",
    "    v2_1.add(tmp);\n",
    "}\n",
    "\n",
    "def p1 = new Plot(title: \"Learning Curve\", yLabel: \"Utility estimates\", xLabel: \"Number of trails\");\n",
    "p1 << new Line(x: T, y: v4_3, displayName: \"v4_3\")\n",
    "p1 << new Line(x: T, y: v3_3, displayName: \"v3_3\")\n",
    "p1 << new Line(x: T, y: v1_3, displayName: \"v1_3\")\n",
    "p1 << new Line(x: T, y: v1_1, displayName: \"v1_1\")\n",
    "p1 << new Line(x: T, y: v3_2, displayName: \"v3_2\")\n",
    "p1 << new Line(x: T, y: v2_1, displayName: \"v2_1\")\n",
    "\n",
    "def trails = [];\n",
    "def rmseValues = [];\n",
    "for (int t = 0; t < rmseTrialsToReport; t++) {\n",
    "    trails.add(t);\n",
    "    double xSsquared = 0;\n",
    "    for (int r = 0; r < numRuns; r++) {\n",
    "        Map<Cell<Double>, Double> u = runs.get(r).get(t);\n",
    "        Double val1_1 = u.get(cw.getCellAt(1, 1));\n",
    "        xSsquared += Math.pow(0.705 - val1_1, 2);\n",
    "    }\n",
    "    double rmse = Math.sqrt(xSsquared/runs.size());\n",
    "    rmseValues.add(rmse);\n",
    "}\n",
    "def p2 = new Plot(yLabel: \"RMS error in utility\", xLabel: \"Number of trails\");\n",
    "p2 << new Line(x: trails, y: rmseValues)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Utility estimates][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/adp_utility_estimates.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![RMS error in utility][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/adp_RMSerror.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first figure shows the utility estimates for some of the states as a function of the number of trails. Notice the large changes occurring around the 63rd trial - this is the first time that the agent falls into the -1 terminal state at $[4,2]$. \n",
    "* The second plot shows the root-mean-square error in the estimate for $U(1,1)$, averaged over 20 runs of 100 trails each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-difference Learning\n",
    "\n",
    "Another way to solve the underlying MDP as in the preceding section is to use the observed transition to adjust the utilities of the observed states so that they agree with the constraint equations. More generally, when a transition occurs from state $s$ to state $s'$, we apply the following update rule to $U^\\pi(s)$:\n",
    "\n",
    "$U^{\\pi}(s) \\leftarrow U^{\\pi}(s)  + \\alpha(R(s) + \\gamma U^\\pi(s') - U^{\\pi}(s) )$\n",
    "\n",
    "Here $\\alpha$ is the **learning rate** parameter. Because this update rule uses the difference in the utilities of the successive states, it is often called the **temporal-difference** equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of passive learning, the equilibrium is given by equation $U^{\\pi}(s) = R(s) + \\gamma\\sum_{s'}P(s' | s,\\pi(s))U^\\pi(s')$. Now the temporal-difference equation does, in fact, cause the agent to reach the equilibrium but notice that the update involves only the observed successor $s'$, whereas the actual equilibrium conditions involve all possible next states. One might think that this causes improperly large changes in $U^\\pi(s)$, but, in fact, because rare transitions occur only rarely, the average value of $U^\\pi(s)$ will converge to the correct value. Furthermore, if we change $\\alpha$ from a fixed parameter to a function that decreases as the number of times a state has been visited increases, then $U^\\pi(s)$ will itself converge to the correct value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the pseudo-code of the Passive TD agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### AIMA3e\n",
       "__function__ Passive-TD-Agent(_percept_) __returns__ an action  \n",
       "&emsp;__inputs__: _percept_, a percept indication the current state _s'_ and reward signal _r'_  \n",
       "&emsp;__persistent__: _&pi;_, a fixed policy  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_U_, a table of utilities, initially empty  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_N<sub>s</sub>_, a table of frequencies for states, initially zero  \n",
       "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;_s_, _a_, _r_, the previous state, action, and reward, initially null  \n",
       "\n",
       "&emsp;__if__ _s'_ is new __then__ _U_[_s'_] &larr; _r'_  \n",
       "&emsp;__if__ _s_ is not null __then__  \n",
       "&emsp;&emsp;&emsp;increment _N<sub>s</sub>_[_s_]  \n",
       "&emsp;&emsp;&emsp;_U_[_s_] &larr; _U_[_s_] + _&alpha;_(_N<sub>s</sub>_[_s_])(r + _&gamma;_ _U_[_s'_] - _U_[_s_])  \n",
       "&emsp;__if__ _s'_.Terminal? __then__ _s_, _a_, _r_ &larr; null __else__ _s_, _a_, _r_ &larr; _s'_, _&pi;_[_s'_], _r'_  \n",
       "&emsp;return _a_\n",
       "\n",
       "---\n",
       "__Figure ??__ A passive reinforcement learning agent that learns utility estimates using temporal differences. The step-size function &alpha;(_n_) is chosen to ensure convergence, as described in the text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "from notebookUtils import *\n",
    "pseudocode('Passive TD Agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this agent on the $4*3$ cell world discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell \t:\tExpected Utility\n",
      "----------------------------------\n",
      "[1,1] \t:\t0.7008970901464446\n",
      "[1,2] \t:\t0.7981190806547613\n",
      "[1,3] \t:\t0.8382389196871616\n",
      "[2,1] \t:\t0.6452315395688609\n",
      "[2,3] \t:\t0.9154049338048013\n",
      "[3,1] \t:\tnull\n",
      "[3,2] \t:\t0.66972055686578\n",
      "[3,3] \t:\t0.9594893300045573\n",
      "[4,1] \t:\tnull\n",
      "[4,2] \t:\t-1.0\n",
      "[4,3] \t:\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aima.core.environment.cellworld.*;\n",
    "import aima.core.learning.reinforcement.agent.PassiveTDAgent;\n",
    "import aima.core.learning.reinforcement.example.CellWorldEnvironment;\n",
    "import aima.core.probability.example.MDPFactory;\n",
    "import aima.core.probability.mdp.impl.ModifiedPolicyEvaluation;\n",
    "import aima.core.util.JavaRandomizer;\n",
    "\n",
    "import java.util.*;;\n",
    "\n",
    "CellWorld<Double> cw = CellWorldFactory.createCellWorldForFig17_1();;\n",
    "CellWorldEnvironment cwe = new CellWorldEnvironment(\n",
    "            cw.getCellAt(1, 1),\n",
    "            cw.getCells(),\n",
    "            MDPFactory.createTransitionProbabilityFunctionForFigure17_1(cw),\n",
    "            new JavaRandomizer());\n",
    "Map<Cell<Double>, CellWorldAction> fixedPolicy = new HashMap<Cell<Double>, CellWorldAction>();\n",
    "fixedPolicy.put(cw.getCellAt(1, 1), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(2, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(2, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(3, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(3, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(3, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(4, 1), CellWorldAction.Left);\n",
    "PassiveTDAgent<Cell<Double>, CellWorldAction> ptda = new PassiveTDAgent<Cell<Double>, CellWorldAction>(fixedPolicy, 0.2, 1.0);\n",
    "cwe.addAgent(ptda);\n",
    "ptda.reset();\n",
    "cwe.executeTrials(10000);\n",
    "System.out.println(\"Cell\"  + \" \\t:\\t\" + \"Expected Utility\");\n",
    "System.out.println(\"----------------------------------\");\n",
    "Map<Cell<Double>, Double> U = ptda.getUtility();\n",
    "for(int i = 1; i<=4; i++){\n",
    "    for(int j = 1; j<=3; j++){\n",
    "        if(i==2 && j==2) continue; //Ignore wall\n",
    "        System.out.println(\"[\" + i + \",\" + j + \"]\"  + \" \\t:\\t\" + U.get(cw.getCellAt(i,j)));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves of the Passive TD agent for the  $4∗3$ cell world are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aima.core.environment.cellworld.*;\n",
    "import aima.core.learning.reinforcement.agent.PassiveTDAgent;\n",
    "import aima.core.learning.reinforcement.example.CellWorldEnvironment;\n",
    "import aima.core.probability.example.MDPFactory;\n",
    "import aima.core.probability.mdp.impl.ModifiedPolicyEvaluation;\n",
    "import aima.core.util.JavaRandomizer;\n",
    "\n",
    "import java.util.*;\n",
    "\n",
    "int numRuns = 20;\n",
    "int numTrialsPerRun = 1000;\n",
    "int rmseTrialsToReport = 100;\n",
    "int reportEveryN = 1;\n",
    "\n",
    "CellWorld<Double> cw = CellWorldFactory.createCellWorldForFig17_1();;\n",
    "CellWorldEnvironment cwe = new CellWorldEnvironment(\n",
    "            cw.getCellAt(1, 1),\n",
    "            cw.getCells(),\n",
    "            MDPFactory.createTransitionProbabilityFunctionForFigure17_1(cw),\n",
    "            new JavaRandomizer());\n",
    "Map<Cell<Double>, CellWorldAction> fixedPolicy = new HashMap<Cell<Double>, CellWorldAction>();\n",
    "fixedPolicy.put(cw.getCellAt(1, 1), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(1, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(2, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(2, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(3, 1), CellWorldAction.Left);\n",
    "fixedPolicy.put(cw.getCellAt(3, 2), CellWorldAction.Up);\n",
    "fixedPolicy.put(cw.getCellAt(3, 3), CellWorldAction.Right);\n",
    "fixedPolicy.put(cw.getCellAt(4, 1), CellWorldAction.Left);\n",
    "PassiveTDAgent<Cell<Double>, CellWorldAction> ptda = new PassiveTDAgent<Cell<Double>, CellWorldAction>(fixedPolicy, 0.2, 1.0);\n",
    "cwe.addAgent(ptda);\n",
    "Map<Integer, List<Map<Cell<Double>, Double>>> runs = new HashMap<Integer, List<Map<Cell<Double>, Double>>>();\n",
    "for (int r = 0; r < numRuns; r++) {\n",
    "    ptda.reset();\n",
    "    List<Map<Cell<Double>, Double>> trials = new ArrayList<Map<Cell<Double>, Double>>();\n",
    "    for (int t = 0; t < numTrialsPerRun; t++) {\n",
    "        cwe.executeTrial();\n",
    "        if (0 == t % reportEveryN) {\n",
    "            Map<Cell<Double>, Double> u = ptda.getUtility();\n",
    "            trials.add(u);\n",
    "        }\n",
    "    }\n",
    "    runs.put(r, trials);\n",
    "}\n",
    "\n",
    "def T = [];\n",
    "def v4_3 = [];\n",
    "def v3_3 = [];\n",
    "def v1_3 = [];\n",
    "def v1_1 = [];\n",
    "def v3_2 = [];\n",
    "def v2_1 = [];\n",
    "double tmp = 0.0;\n",
    "for (int t = 0; t < (numTrialsPerRun/reportEveryN); t++) {\n",
    "    T.add(t);\n",
    "    Map<Cell<Double>, Double> u = runs.get(numRuns - 1).get(t);\n",
    "    tmp = (u.containsKey(cw.getCellAt(4, 3)) ? u.get(cw.getCellAt(4, 3)) : 0.0);\n",
    "    v4_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(3, 3)) ? u.get(cw.getCellAt(3, 3)) : 0.0);\n",
    "    v3_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(1, 3)) ? u.get(cw.getCellAt(1, 3)) : 0.0);\n",
    "    v1_3.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(1, 1)) ? u.get(cw.getCellAt(1, 1)) : 0.0);\n",
    "    v1_1.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(3, 2)) ? u.get(cw.getCellAt(3, 2)) : 0.0);\n",
    "    v3_2.add(tmp);\n",
    "    tmp = (u.containsKey(cw.getCellAt(2, 1)) ? u.get(cw.getCellAt(2, 1)) : 0.0);\n",
    "    v2_1.add(tmp);\n",
    "}\n",
    "\n",
    "def p1 = new Plot(title: \"Learning Curve\", yLabel: \"Utility estimates\", xLabel: \"Number of trails\");\n",
    "p1 << new Line(x: T, y: v4_3, displayName: \"v4_3\")\n",
    "p1 << new Line(x: T, y: v3_3, displayName: \"v3_3\")\n",
    "p1 << new Line(x: T, y: v1_3, displayName: \"v1_3\")\n",
    "p1 << new Line(x: T, y: v1_1, displayName: \"v1_1\")\n",
    "p1 << new Line(x: T, y: v3_2, displayName: \"v3_2\")\n",
    "p1 << new Line(x: T, y: v2_1, displayName: \"v2_1\")\n",
    "\n",
    "def trails = [];\n",
    "def rmseValues = [];\n",
    "for (int t = 0; t < rmseTrialsToReport; t++) {\n",
    "    trails.add(t);\n",
    "    double xSsquared = 0;\n",
    "    for (int r = 0; r < numRuns; r++) {\n",
    "        Map<Cell<Double>, Double> u = runs.get(r).get(t);\n",
    "        Double val1_1 = u.get(cw.getCellAt(1, 1));\n",
    "        xSsquared += Math.pow(0.705 - val1_1, 2);\n",
    "    }\n",
    "    double rmse = Math.sqrt(xSsquared/runs.size());\n",
    "    rmseValues.add(rmse);\n",
    "}\n",
    "def p2 = new Plot(yLabel: \"RMS error in utility\", xLabel: \"Number of trails\");\n",
    "p2 << new Line(x: trails, y: rmseValues)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Utility estimates][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/td_utility_estimates.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![RMS error in utility][1]][1]\n",
    "\n",
    "[1]: assets/reinforcement_learning/td_RMSerror.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first figure shows the utility estimates for some of the states as a function of the number of trails.\n",
    "* The second plot shows the root-mean-square error in the estimate for $U(1,1)$, averaged over 20 runs of 1000 trails each. Only the first 100 trails are shown to enable comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The passive TD agent does not learn quite as fast as the ADP agent and shows much higher variability, but it is much simpler and requires much lesser computation per observation. Notice that, the TD does not need the transition model to perform its updates. The environment supplies the connection between neighboring states int the form of observed transitions.\n",
    "\n",
    "The ADP approach and the TD approach are closely related. Both try to make local adjustments to the utility estimates in order to make each state \"agree\" with its successors. One difference is TD adjusts a state to agree with its _observed_ successor, whereas ADP adjusts the state to agree with _all_ of the successors that might occur, weighted by their probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Groovy",
   "language": "groovy",
   "name": "groovy"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": ".groovy",
   "mimetype": "",
   "name": "Groovy",
   "nbconverter_exporter": "",
   "version": "2.4.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
